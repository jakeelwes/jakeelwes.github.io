<html>
<head>
    <link rel="stylesheet" href="about.css">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

</head>

<body>

    <img
      src="public/img/curtain-side-full.png"
      class="curtain"
      draggable="false"
    />

    <img
      src="public/img/about.png"
      class="secondary-header"
      draggable="false"
    />
    <img
    src="public/img/About/Zizi banner.jpg"
    class="banner"
    draggable="false"
  />
  <div class="about">

    <hr class="first">
    <h2>Team</h2>
    <ul>
      <li>
        <b>Jake Elwes</b> - Artist
      </li>
      <li>
        <b>Me</b> - Performance Director
      </li>
      <li>
        <b>Alexander Hill</b> - Web and streaming technology &amp;
        development
      </li>
      <li>
        <b>Toby Elwes</b> - Camera
      </li>
      <li>
        <b>Edinburgh Futures</b> - Institute:
      </li>
      <li>
        <b>Drew Hemment</b> - Curator
      </li>
      <li>
        <b>Suzy Glass</b> - Producer
      </li>
    </ul>
    <hr>
    <i><p>
      "My latest work aims to bring together two things I love, artificial
      intelligence, and the world of Drag performance. In an entertaining
      and humorous way Drag has allowed me to dig into some of the social
      issues built into machine learning technology.
    </p>
    <p>
      Drag is a brilliant lens through which we can explore and expose the layers of technical construction and social bias inherent to AI.
    </p>
    <p>
      Working closely with friends from the London drag scene, in Zizi we have created a ‘deep-fake’ virtual cabaret. This deep-fake tech has enabled us to collaborate with machine learning to do drag, demonstrating how drag queens, drag kings and drag things can never be replaced by artificial intelligence.
    </p>
    <p>
      The Zizi Project pushes the boundaries of both drag and AI to discover what AI can teach us about drag – and what drag can teach us about AI.”
    </p>
  </i>
    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Jake Elwes</p>
    <hr>

    <h2>Technical</h2>
    
    <p>
      Machine learning (teaching computers to learn from data), more specifically deep-fake technology, has been used to construct all the videos you see on this website. To produce a deep-fake, you start by training a neural network<sup><a href="#fn1" id="ref1">1</a></sup> on a dataset of images.
    </p>
    <p>
      This dataset contains the original images (video frames) of the real-life person as well as a graphic tracking the position of their skeleton, facial features, and silhouette. 
    </p>
    <p>
      Creating deep-fakes begins with training a neural network to try to recreate the original image of this person, from only seeing their skeleton tracking (illustrated below). The neural network aims to get as close as possible to the original and does this by being given an accuracy score.<sup><a href="#fn2" id="ref2">2</a></sup> Once it has learnt to do this it can then start producing deep-fakes.
    </p>
    <p>
      Below you can see the iterative training process of a neural network learning how to create images of Lilly SnatchDragon:
    </p>

    <img src="public/img/About/diagram.gif" draggable="false" />

    <p>
      Using machine learning, this process iterates and improves until it can create new, fake faces which are indistinguishable from the real. For Zizi the method I use is called Video-to-Video Synthesis.<sup><a href="#fn3" id="ref3">3</a></sup>
    </p>
    Once the neural network has been training for, let's say, three days, it is ready to be fed new movements. Anyone can now control the deep-fake body by running skeleton tracking on a new video and then feeding these into the neural network.
    </p>
    <p>
      These visuals below show how new deep-fake images of Lilly SnatchDragon can be generated from the trained neural network (here with her movement controlled by Me the Drag Queen).
    </p>

    <img src="public/img/About/diagram-gen-close-small.gif" draggable="false" />
    <img src="public/img/About/diagram-gen-full-small.gif" draggable="false" />
    <p>
      This process was repeated to create deep fakes of all 13 of our wonderful, diverse drag cast. 
    </p>
    <p>
      The ‘Zizi’ character was created by simultaneously training on images of all of the performers. Not knowing how to differentiate between the bodies, the result is an amalgamation, a ‘queering’ of the data. 
    </p>
    <p>
      Facial recognition algorithms (and deep fake technology) currently have a real problem recognising trans and non-binary people, as well as other marginalised identities, because they’ve been trained on photos of people with cis binary identities. <sup><a href="#fn4" id="ref4">4</a></sup><sup><a href="#fn5" id="ref5">5</a></sup>
    </p>
    <p>
      The project poses the question whether making deep fakes using queer identities becomes a means of assimilation or inclusivity… or more a techno-activist method of dirtying and obfuscating the systems used to collect data on us.
    </p>
    <p>
      The Zizi project aims to critically examine these techniques using a dataset of drag performers, in the process exposing the workings of the black box which is artificial intelligence.
    </p>
    <hr>
    

    <h2>Footnotes</h2>
    <p><sup class="footnote" id="fn1">1. For more information see https://deepai.org/machine-learning-glossary-and-terms/neural-network<a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a></sup>
  </p><p>

    <sup class="footnote" id="fn2">2. an accuracy score calculated using the gradient descent learning algorithm<a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a></sup>
  </p><p>

    <sup class="footnote" id="fn3">3. Video-to-Video Synthesis is a conditional generative adversarial network (cGAN) developed by Wang et al. (Nvidia &amp; MIT), NeurIPS, 2018. It uses OpenPose (2018) for skeleton tracking and DensePose (2018) for silhouette estimation. This technique also uses Flownet (2016) to take into account the motion in the video.<a href="#ref3" title="Jump back to footnote 3 in the text.">↩</a></sup>
  </p><p>

    <sup class="footnote" id="fn4">4. See Hamidi, F., Scheuerman, M.K. and Branham, S.M., 2018, April. Gender recognition or gender reductionism? The social implications of embedded gender recognition systems. In Proceedings of the 2018 chi conference on human factors in computing systems (pp. 1-13).<a href="#ref4" title="Jump back to footnote 4 in the text.">↩</a></sup>
  </p><p>

    <sup class="footnote" id="fn5">5. Excavating AI: The Politics of Images in Machine Learning Training Sets, Kate Crawford and Trevor Paglen    https://www.excavating.ai/<a href="#ref5" title="Jump back to footnote 5 in the text.">↩</a></sup>
  </p>

    <hr>


    <p>
      The Zizi Project (2019 - ongoing) is a collection of works by Jake
      Elwes exploring the intersection of Artificial Intelligence (A.I.) and
      drag performance. Drag challenges gender and explores otherness, while
      A.I. is often mystified as a tool and contains social bias. Zizi
      combines them through a deep fake, synthesised drag identity created
      using machine learning. The project explores what AI can teach us
      about drag, and what drag can teach us about A.I.
    </p>
    <hr>
    <h2>'Zizi &amp; Me' 2020</h2>
    <iframe
      width="100%"
      src="https://www.youtube.com/embed/vtpVr5KVvnk"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowFullScreen
    ></iframe>
    <h2>'Zizi: Queering the Dataset' 2019</h2>
    <iframe
      src="https://player.vimeo.com/video/388245510"
      width="100%"
      frameborder="0"
      allow="autoplay; fullscreen"
      allowFullScreen
    ></iframe>
    <h2>National Gallery X + FLUX - 2020</h2>
    <iframe
      width="100%"
      src="https://www.youtube.com/embed/QOK97wutH-s"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowFullScreen
    ></iframe>
    <hr>
  </div>

</body>

</html>