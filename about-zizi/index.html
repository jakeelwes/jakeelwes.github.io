<html>

<head>
	<!-- <link rel="stylesheet" href="about.css"> -->
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap"
		rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"
		integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw=="
		crossorigin="anonymous" referrerpolicy="no-referrer" />
	<link rel="stylesheet" href="output.css">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body class="bg-black text-zinc-300">
	<header class="w-full bg-cover bg-no-repeat bg-center bg-top h-96"
		style="background-image: url('img/banner-tall.jpg')">
		<div class="w-full h-96 bg-gradient-to-t from-black to-60%">
			<!-- <div class="text-[5rem] text-center w-full h-full">
				<a class="text-white no-underline hover:no-underline visited:text-white mt-32 fa-regular fa-circle-play"
					href="https://www.youtube.com/watch?v=3c5-ABUkI_M"></a>
			</div> -->
		</div>
	</header>
	<main class="w-full md:container md:mx-auto">
		<div class="px-8">
			<h1 class="text-3xl -mt-24 text-zinc-50 font-title">The Zizi Show</h1>
			<p class="italic">A Deepfake Drag Cabaret</p>
			<iframe class="w-full aspect-video my-8" src="https://www.youtube.com/embed/3c5-ABUkI_M"
				title="Making of The Zizi Show" frameborder="0"
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
				allowfullscreen></iframe>
			<!-- <ul>
			<li>
				Videos
			</li>
			<li>
				The Team
			</li>
			<li>
				Intro
			</li>
			<li>
				The Process | <small><small>footnotes</small></small>
			</li>
			<li>
				About the Artist
			</li>
			<li>
				About the Zizi Project
			</li>
			<li>
				The Zizi Project: Other Works
			</li>
		</ul> -->
			<div class="p-4 italic space-y-4 font-serif">
				<p>
					"My latest work aims to bring together two things I love, artificial
					intelligence, and the world of Drag performance. In an entertaining
					and humorous way Drag has allowed me to dig into some of the social
					issues built into machine learning technology.
				</p>
				<p>
					Drag is a brilliant lens through which we can explore and expose the layers of technical
					construction
					and social bias inherent to AI.
				</p>
				<p>
					Working closely with friends from the London drag scene, in Zizi we have created a ‘deep-fake’
					virtual
					cabaret. This deep-fake tech has enabled us to collaborate with machine learning to do drag,
					demonstrating how drag queens, drag kings and drag things can never be replaced by artificial
					intelligence.
				</p>
				<p>
					The Zizi Project pushes the boundaries of both drag and AI to discover what AI can teach us about
					drag –
					and what drag can teach us about AI.”
				</p>
				<p class="text-right text-2xl font-serif">- Jake Elwes</p>
			</div>
		</div>

		<div class="w-full bg-zinc-700 p-8 text-center md:rounded-lg">
			<h2 class="font-bold text-xl mb-8">Team</h2>
			@Jake Todo: add links, update descriptions
			<ul>
				<li>
					<b>Jake Elwes</b> - Artist
				</li>
				<li>
					<b>Me</b> - Performance Director
				</li>
				<li>
					<b>Alexander Hill</b> - Web and streaming technology &amp; development
				</li>
				<li>
					<b>Toby Elwes</b> - Camera
				</li>
				<li>
					<b>Edinburgh Futures</b> - Institute:
				</li>
				<li>
					<b>Drew Hemment</b> - Curator
				</li>
				<li>
					<b>Suzy Glass</b> - Producer
				</li>
			</ul>
		</div>

		<div class="w-full bg-zinc-100 p-8 text-center text-red-950 md:rounded-lg md:my-6">
			<h2 class="font-bold text-xl">Cast</h2>
			<div id="cast-image-grid" class="container mx-auto grid grid-cols-2 gap-4 md:grid-cols-3">
				<a class="flex flex-col items-center" href="https://instagram.com" target="_blank">
					<!-- <i class="self-end fa-brands fa-instagram"></i> -->
					<img src="img/cast/bolly.jpg" alt="Bolly Illusion Cast Photo" class="rounded-lg">
					<p class="underline decoration-2 decoration-red-700 underline-offset-2">Bolly Illusion</p>
				</a>
				<div class="flex flex-col items-center">
					<!-- <i class="right-1 top-1 fa-brands fa-instagram"></i> -->
					<img src="img/cast/cara.jpg" alt="Cara Melle Cast Photo" class="rounded-lg">
					<p class="underline decoration-2 decoration-orange-700 underline-offset-2">Cara Melle</p>
				</div>
				<div class="flex flex-col items-center">
					<!-- <i class="right-1 top-1 fa-brands fa-instagram"></i> -->
					<img src="img/cast/chiyo.jpg" alt="Chiyo Cast Photo" class="rounded-lg">
					<p class="underline decoration-2 decoration-lime-600 underline-offset-2">Chiyo</p>
				</div>
				<div class="flex flex-col items-center">
					<!-- <i class="right-1 top-1 fa-brands fa-instagram"></i> -->
					<img src="img/cast/dakota.jpg" alt="Dakota Cast Photo" class="rounded-lg">
					<p class="underline decoration-2 decoration-teal-700 underline-offset-2">Dakota</p>
				</div>
				<div class="flex flex-col items-center">
					<!-- <i class="right-1 top-1 fa-brands fa-instagram"></i> -->
					<img src="img/cast/mahatma.jpg" alt="Mahatma Khandi Cast Photo" class="rounded-lg">
					<p class="underline decoration-2 decoration-fuchsia-700 underline-offset-2">Mahatma Khandi</p>
				</div>
				<div class="flex flex-col items-center">
					<!-- <i class="right-1 top-1 fa-brands fa-instagram"></i> -->
					<img src="img/cast/mark.jpg" alt="Dahc Dermur Viii Cast Photo" class="rounded-lg">
					<p class="underline decoration-2 decoration-pink-700 underline-offset-2">Dahc Dermur Viii</p>
				</div>
			</div>
		</div>

		<div class="w-full px-8 mt-16 space-y-6">
			<h2 class="text-2xl mb-2 font-mono text-center">Technical</h2>
			<p>
				Machine learning (teaching computers to learn from data), more specifically deep-fake technology, has
				been
				used to construct all the videos you see on this website. To produce a deep-fake, you start by training
				a
				neural network<sup><a href="#fn1" id="ref1">1</a></sup> on a dataset of images.
			</p>
			<p>
				This dataset contains the original images (video frames) of the real-life person as well as a graphic
				tracking the position of their skeleton, facial features, and silhouette.
			</p>
			<p>
				Creating deep-fakes begins with training a neural network to try to recreate the original image of this
				person, from only seeing their skeleton tracking (illustrated below). The neural network aims to get as
				close as possible to the original and does this by being given an accuracy score.<sup><a href="#fn2"
						id="ref2">2</a></sup> Once it has learnt to do this it can then start producing deep-fakes.
			</p>
			<p>
				Below you can see the iterative training process of a neural network learning how to create images of
				Lilly
				SnatchDragon:
			</p>

			<img class="mx-auto" src="img/diagram.gif" draggable="false" />

			<p>
				Using machine learning, this process iterates and improves until it can create new, fake faces which are
				indistinguishable from the real. For Zizi the method I use is called Video-to-Video Synthesis.
				<sup><a href="#fn3" id="ref3">3</a></sup>
			</p>
			<p>
				Once the neural network has been training for, let's say, three days, it is ready to be fed new
				movements.
				Anyone can now control the deep-fake body by running skeleton tracking on a new video and then feeding
				these
				into the neural network.
			</p>
			<p>
				These visuals below show how new deep-fake images of Lilly SnatchDragon can be generated from the
				trained
				neural network (here with her movement controlled by Me the Drag Queen).
			</p>

			<img class="mx-auto" src="img/diagram-gen-close-small.gif" draggable="false" />
			<img class="mx-auto" src="img/diagram-gen-full-small.gif" draggable="false" />
			<p>
				This process was repeated to create deep fakes of all 13 of our wonderful, diverse drag cast.
			</p>
			<p>
				The ‘Zizi’ character was created by simultaneously training on images of all of the performers. Not
				knowing
				how to differentiate between the bodies, the result is an amalgamation, a ‘queering’ of the data.
			</p>
			<p>
				Facial recognition algorithms (and deep fake technology) currently have a real problem recognising trans
				and
				non-binary people, as well as other marginalised identities, because they’ve been trained on photos of
				people with cis binary identities.<sup><a href="#fn4" id="ref4">4</a>,<a href="#fn5"
						id="ref5">5</a></sup>
			</p>
			<p>
				The project poses the question whether making deep fakes using queer identities becomes a means of
				assimilation or inclusivity… or more a techno-activist method of dirtying and obfuscating the systems
				used
				to collect data on us.
			</p>
			<p>
				The Zizi project aims to critically examine these techniques using a dataset of drag performers, in the
				process exposing the workings of the black box which is artificial intelligence.
			</p>

			<div class="space-y-4 text-xs footnotes">
				<h3 class="italic">Footnotes</h3>
				<p id="fn1">1. For more information see
					<a href="https://deepai.org/machine-learning-glossary-and-terms/neural-network">this article from
						deepai.</a><a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a>
				</p>

				<p id="fn2">2. An accuracy score calculated using the gradient descent learning
					algorithm<a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a></p>

				<p id="fn3">3. Video-to-Video Synthesis is a conditional generative adversarial
					network
					(cGAN) developed by Wang et al. (Nvidia &amp; MIT), NeurIPS, 2018. It uses OpenPose (2018) for
					skeleton
					tracking and DensePose (2018) for silhouette estimation. This technique also uses Flownet (2016) to
					take
					into account the motion in the video.<a href="#ref3"
						title="Jump back to footnote 3 in the text.">↩</a>
				</p>
				<p id="fn4">4. See Hamidi, F., Scheuerman, M.K. and Branham, S.M., 2018, April.
					Gender
					recognition or gender reductionism? The social implications of embedded gender recognition systems.
					In Proceedings of the 2018 chi conference on human factors in computing systems (pp. 1-13).<a
						href="#ref4" title="Jump back to footnote 4 in the text.">↩</a>
				</p>
				<p id="fn5">5. Excavating AI: The Politics of Images in Machine Learning Training
					Sets,
					Kate Crawford and Trevor Paglen https://www.excavating.ai/<a href="#ref5"
						title="Jump back to footnote 5 in the text.">↩</a>
				</p>
			</div>
		</div>

		<div class="w-full p-8">
			<h2>MORE CONTENT!!!!</h2>
			<p>
				The Zizi Project (2019 - ongoing) is a collection of works by Jake
				Elwes exploring the intersection of Artificial Intelligence (A.I.) and
				drag performance. Drag challenges gender and explores otherness, while
				A.I. is often mystified as a tool and contains social bias. Zizi
				combines them through a deep fake, synthesised drag identity created
				using machine learning. The project explores what AI can teach us
				about drag, and what drag can teach us about A.I.
			</p>
			<h2>'Zizi &amp; Me' 2020</h2>
			<!-- <iframe class="w-full aspect-video" src="https://www.youtube.com/embed/vtpVr5KVvnk"
				title="YouTube video player" frameborder="0"
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
				allowfullscreen></iframe> -->
			<h2>'Zizi: Queering the Dataset' 2019</h2>
			<!-- <iframe src="https://player.vimeo.com/video/388245510" frameborder="0"
				allow="autoplay; fullscreen" allowFullScreen></iframe> -->
			<h2>National Gallery X + FLUX - 2020</h2>
			<!-- <iframe src="https://www.youtube.com/embed/QOK97wutH-s" frameborder="0"
				allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
				allowFullScreen></iframe> -->
		</div>
	</main>

</body>

</html>